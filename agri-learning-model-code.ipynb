{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple CNN Model","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nfrom torchvision.transforms import transforms\nfrom PIL import Image\nimport numpy as np\nimport os\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport gc\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=[]\nfeatures=[]\nfor dirname, _, filenames in os.walk('../input/plantdisease/PlantVillage'): #PlantVillage Dataset Path\n    for filename in filenames:\n        features.append(filename)\n        labels.append(dirname)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_sketches():\n    TRANSFORM_IMG = transforms.Compose([\n        transforms.Resize((225, 225)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n    DATA_DIR = \"../input/plantdisease/PlantVillage\" \n    train_data = datasets.ImageFolder(DATA_DIR, transform=TRANSFORM_IMG)\n    test_data = datasets.ImageFolder(DATA_DIR, transform=TRANSFORM_IMG)\n    num_train = len(train_data)\n    indices = list(range(num_train))\n    valid_size = .2\n    split = int(np.floor(valid_size * num_train))\n    np.random.shuffle(indices)\n    train_idx, test_idx = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_idx)\n    test_sampler = SubsetRandomSampler(test_idx)\n    trainloader = DataLoader(train_data,\n                   sampler=train_sampler, batch_size=64)\n    testloader = DataLoader(test_data,\n                   sampler=test_sampler, batch_size=64)\n    return trainloader, testloader","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_sketches()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\nnum_classes = 15 \nbatch_size = 64\nlearning_rate = 0.001\n\nprint(num_epochs, num_classes, batch_size, learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Sketch_a_Net_CNN(torch.nn.Module):\n    def __init__(self):\n        super(Sketch_a_Net_CNN, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=15, stride=3, padding=0), #kernel size is filter size\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        )\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=5, stride=1 , padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        )\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, stride=1 , padding=1),\n            nn.ReLU()\n        )\n        \n        self.layer4 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1 , padding=1),\n            nn.ReLU()\n        )\n\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, stride=1 , padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        )\n\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=7, stride=1 , padding=0),\n            nn.ReLU(),\n            nn.Dropout(p=0.5)\n        )\n\n\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=1, stride=1 , padding=0),\n            nn.ReLU(),\n            nn.Dropout(p=0.5)\n        )\n        \n        self.fc1 = nn.Sequential(\n            nn.Linear(512, num_classes)\n        )\n\n         \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = self.layer6(out)\n        out = self.layer7(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc1(out)\n        return(out)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = Sketch_a_Net_CNN().to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    train_loader, val_loader = get_sketches()\n\n    total_step = len(train_loader)\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(tqdm(train_loader), 1):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        torch.save(model.state_dict(), \"model.%d\" % epoch)\n\n        model.eval()\n\n        train_correct = 0\n        train_five_correct = 0\n        train_total = 0\n\n        \n        with torch.no_grad():\n            for data in tqdm(train_loader):\n                images, labels = data\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                _, predicted_five = torch.topk(outputs.data, 5, dim=1)\n\n                train_total += labels.size(0)\n\n                train_correct += (predicted == labels).sum().item()\n        \n        print('Top One Error of the network on train images: %d %%' % (\n                100 * (1 - train_correct / train_total)))\n\n       # print('Top Five Error of the network on train images: %d %%' % (\n        #   100 * (1 - train_five_correct /train_total)))\n\n        correct = 0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for data in tqdm(val_loader):\n                images, labels = data\n\n                images = images.to(device)\n                labels = labels.to(device)\n\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                _, predicted_five = torch.topk(outputs.data, 5, dim=1)\n\n                val_total += labels.size(0)\n\n                correct += (predicted == labels).sum().item()\n        \n\n        print('Top One Error of the network on validation images: %d %%' % (\n                100 * (1 - correct / val_total)))\n\n       # print('Top Five Error of the network on validation images: %d %%' % (\n        #   100 * (1 - val_correct / val_total)))\n        \n        gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PATH = \"./model.22\" \nTRAINING_IMAGES_DIRECTORY = \"../input/plantdisease/PlantVillage\"\ntesting_image = \"../input/testingdata/tomato leaf mold.jpg\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sketchnet():\n    model = Sketch_a_Net_CNN()\n    return model\ndef load_model():\n    model = sketchnet()\n    model_path = MODEL_PATH \n    checkpoint = torch.load(model_path)\n    model.load_state_dict(checkpoint)\n    return model\ndef construct_transformer():\n    \"\"\"construct transformer for images\"\"\"\n    transformer = transforms.Compose([\n        #transforms.ToPILImage(),\n        transforms.Resize((225, 225)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    return transformer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_categories():\n    \"\"\"load the classification id-name dictionary\"\"\"\n    classes = os.listdir(TRAINING_IMAGES_DIRECTORY) \n    classes.sort()\n    class_to_idx = {i: classes[i] for i in range(len(classes))}\n    return class_to_idx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(image_path):\n    # load classification categories\n    categories = load_categories()\n    print(categories)\n    # setup the device for running\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # load model and set to evaluation mode\n\n    model = load_model()\n    model.to(device)\n    model.eval()\n\n    # set image transformer\n    transformer = construct_transformer()\n    image = Image.open(image_path)\n    image = image.convert('RGB')\n    \n    image = transformer(image)\n    image = image.view([1, 3, 225, 225])\n    image = image.to(device)\n\n\n    # run the forward process\n    prediction = model(image)\n    _, cls = torch.max(prediction, dim=1)\n\n    # output\n    index = cls.data.tolist()[0]\n    print()\n    print(index)\n\n    # Assign PRED_CATEGORY to correct value\n    pred_category = categories[index] #\"Drew (and not the one from Despicable Me!)\"\n    print(\"The predicted category is \" + pred_category)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main(tesing_image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VGG-16 Model from scratch","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport keras,os\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D , Flatten\nfrom keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport keras.models\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dataframe=pd.read_csv(\"../input/dataframe-final/Dataframe_final (1).csv\")\ntrain_df, test_df = train_test_split(Dataframe, test_size=0.2)\ndatagen=ImageDataGenerator(rescale=1./255.)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator=datagen.flow_from_dataframe(\ndataframe=train_df,\nx_col=\"Plant disease\",\ny_col=\"Plant type\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(224,224))\n\nvalid_generator=datagen.flow_from_dataframe(\ndataframe=test_df,\nx_col=\"Plant disease\",\ny_col=\"Plant type\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(224,224))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(units=4096,activation=\"relu\"))\nmodel.add(Dense(units=15, activation=\"softmax\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\nhist=model.fit_generator(generator=train_generator,\n                    steps_per_epoch=100,\n                    validation_data=valid_generator,\n                    validation_steps=10,\n                    callbacks=[checkpoint,early],\n                    epochs=100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist.history[\"accuracy\"])\nplt.plot(hist.history['val_accuracy'])\nplt.title(\"model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Accuracy\",\"Validation Accuracy\"])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-trained VGG-16 Model","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport pandas as pd\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.preprocessing.image import ImageDataGenerator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg=VGG16(include_top=False,weights='imagenet',input_shape=(100,100,3))\n\nfor layer in vgg.layers:\n   layer.trainable = False\n\nx=Flatten()(vgg.output)\nprediction=Dense(15,activation='softmax')(x)\nmodel=tf.keras.Model(inputs=vgg.input,outputs=prediction)\n\nmodel.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Dataframe=pd.read_csv(\"../input/dataframeeeeeeeee/Dataframe_final (1).csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(Dataframe, test_size=0.2)\ndatagen=ImageDataGenerator(rescale=1./255.)  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator=datagen.flow_from_dataframe(\ndataframe=train_df,\nx_col=\"Plant disease\",\ny_col=\"Plant type\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(100,100))\n\nvalid_generator=datagen.flow_from_dataframe(\ndataframe=test_df,\nx_col=\"Plant disease\",\ny_col=\"Plant type\",\nbatch_size=32,\nseed=42,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(100,100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_files=train_df.shape[0]\nvalid_files=test_df.shape[0]\nbatch_size=32\nepochs=50","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = model.fit_generator(\n train_generator,\n validation_data=valid_generator,\n epochs=epochs,\n steps_per_epoch=image_files//batch_size,\n validation_steps=valid_files//batch_size,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grad-CAM on Pre-trained VGG-16 Model","metadata":{}},{"cell_type":"code","source":"from PIL import Image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = \"../input/plantdisease/PlantVillage/Tomato_Early_blight/1742dfdd-c687-46ff-b958-34194feb68e4___RS_Erly.B 7453.JPG\"\nimg = Image.open(image_path)\nnewsize = (100, 100)\nimg = img.resize(newsize)\nimg.save('7.jpg')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.load_model('../input/vggpretrained-94acc/model (3).h5') #best one","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = (100,100)\ndecode_predictions = keras.applications.vgg16.decode_predictions\nlast_conv_layer_name = \"block5_conv3\"\nimg_path='./7.jpg'\ndisplay(Image(img_path))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_img_array(img_path, size):\n    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = keras.preprocessing.image.img_to_array(img)\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))n\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array = get_img_array(img_path, size=img_size)\nmodel=model\nmodel.layers[-1].activation = None\nheatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\nplt.matshow(heatmap)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    img = keras.preprocessing.image.load_img(img_path)\n    img = keras.preprocessing.image.img_to_array(img)\n    heatmap = np.uint8(255 * heatmap)\n    jet = cm.get_cmap(\"jet\")\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    superimposed_img.save(cam_path)\n    display(Image(cam_path))\n\nsave_and_display_gradcam(img_path, heatmap)","metadata":{},"execution_count":null,"outputs":[]}]}